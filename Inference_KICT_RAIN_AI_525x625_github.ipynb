{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf1f694-030f-4bb4-b281-59659901eabe",
   "metadata": {},
   "source": [
    "#목적형 R&R 경안천 유역 대상 적용을 위한 예측 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98371e80-0af2-490e-bd9a-8a51d62e5edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 17:00:44.152130: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import  train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import gc\n",
    "from PIL import Image\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87dab69-d2b3-49d9-be9f-6ef4029162a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version : 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras.metrics import AUC\n",
    "\n",
    "print(f\"Keras Version : {keras.__version__}\")\n",
    "\n",
    "def mae255(y_true, y_pred):\n",
    "    return 255 * mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f39d8c5-24f1-45a8-b7e0-a2523e5234a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# Metrics\n",
    "# f1 score 계산\n",
    "def Rain(array):\n",
    "    R=(array*255.)\n",
    "    return R\n",
    "\n",
    "def CSI_m(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true > 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    right=np.sum(y_true * y_pred == 1)\n",
    "    #print(right,np.sum(y_pred),np.sum(y_true),right)\n",
    "    CSI = right/(np.sum(y_pred)+np.sum(y_true)-right+1e-07)\n",
    "    return CSI\n",
    "\n",
    "def CSI_custom(y_true, y_pred):\n",
    "    score = tf.py_function(func=CSI_m, inp=[y_true, y_pred], Tout=tf.float32,  name='CSI_custom') \n",
    "    return score\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred): \n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def mae_custom(y_true, y_pred):\n",
    "    y_pred=tf.convert_to_tensor(y_pred)\n",
    "    y_true=tf.convert_to_tensor(y_true)\n",
    "    y_true = Rain(y_true)\n",
    "    y_pred = Rain(y_pred)  \n",
    "    thr1 = K.greater(y_true,0.1)\n",
    "    loss1 =K.mean(K.abs(y_true[thr1] - y_pred[thr1]))\n",
    "    arr=[loss1]    \n",
    "    value_not_nan = tf.dtypes.cast(tf.math.logical_not(tf.math.is_nan(arr)), dtype=tf.float32)\n",
    "    loss0=tf.math.multiply_no_nan(arr, value_not_nan)\n",
    "    loss=tf.convert_to_tensor(K.sum(loss0))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a2417-81cf-4ffe-8161-1bf4759680da",
   "metadata": {},
   "source": [
    "#특정시점 예측해서 이미지 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5312ef05-1d68-4647-8a14-31d59a479666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208082200\n"
     ]
    }
   ],
   "source": [
    "date_nm='202208082200'\n",
    "print(date_nm[2:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2de5a3d-1ae4-4d76-b0e6-f395c93b4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_loader for QPF\n",
    "# READ HFC_COMP_COCM (PAST 4 data)\n",
    "def read_ASC(timestamp):\n",
    "#    print(timestams)\n",
    "    YY = timestamp[0:2]\n",
    "    MM = timestamp[2:4]\n",
    "    DD = timestamp[4:6]\n",
    "    HH = timestamp[6:8]\n",
    "    MN = timestamp[8:10]\n",
    "    RDR_time = timestamp[0:10]\n",
    "\t#RAD200621192000 = 20200621192000\n",
    "    YYYY = '20'+YY\n",
    "    print(RDR_time)\n",
    "    #path='/data/lapisis_data/RDR_COMP/INPUT_ASC'\n",
    "    path='/data'\n",
    "    filename = path+'/RDR_CMP_HSP_PUB_20'+RDR_time+'-525x625-1km.asc'\n",
    "    data = np.zeros([625,525], np.float32) #ASCII랑배열 반대\n",
    "    data = np.loadtxt(filename,skiprows=6)\n",
    "    data[data < 0] = 0.\n",
    "    #data_re=data[46:302,186:442]*6. #256x256\n",
    "    attr = datetime.datetime(int(YYYY),int(MM),int(DD),int(HH),int(MN))\n",
    "    return data, attr\n",
    "\n",
    "def qpe_data(filetime):\n",
    "    YY = filetime[0:2]\n",
    "    MM = filetime[2:4]\n",
    "    DD = filetime[4:6]\n",
    "    HH = filetime[6:8]\n",
    "    MN = filetime[8:10]\n",
    "    YYYY = '20'+YY\n",
    "    latest_datetime = datetime.datetime(int(YYYY),int(MM),int(DD),int(HH),int(MN))\n",
    "    #print(latest_datetime)\n",
    "    list_for_qpf = [ts.strftime(\"%y%m%d%H%M\") for ts in \n",
    "    [latest_datetime - datetime.timedelta(minutes=t) for t in [30, 20, 10, 0]]]\n",
    "    input_scans = np.array([read_ASC(ts)[0] for ts in list_for_qpf])\n",
    "    qpe_scans = np.concatenate([input_scans], axis=0)\n",
    "    print(qpe_scans.shape)\n",
    "    return qpe_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8d9715-f5bc-49a7-b526-65633995b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X):\n",
    "   # 0. Right shape for batch\n",
    "   X = X[np.newaxis, ::, ::, ::]\n",
    "   return X\n",
    "\n",
    "def data_postprocessing(nwcst):\n",
    "    # 0. Squeeze empty dimensions\n",
    "    nwcst = np.squeeze(np.array(nwcst))\n",
    "    nwcst = nwcst\n",
    "    nwcst = np.where(nwcst>0, nwcst, 0)\n",
    "    return nwcst   \n",
    "\n",
    "def prediction(model_instance, input_data):\n",
    "    input_data = data_preprocessing(input_data)\n",
    "    print(input_data.shape)\n",
    "    pred = model_instance.predict(input_data)\n",
    "    print(pred.shape)\n",
    "    nwcst = np.squeeze(pred)  # np.transpose() 대신 np.squeeze()를 사용하세요.\n",
    "    return nwcst\n",
    "\n",
    "def save_as_ascii_grid(data, output_file):\n",
    "    \"\"\"\n",
    "    2D NumPy array를 ASCII grid 파일로 저장하는 함수\n",
    "    :param data: 2D NumPy array (격자 자료)\n",
    "    :param output_file: 저장할 파일 경로 및 이름 (예: 'output.asc')\n",
    "    \"\"\"\n",
    "    nrows, ncols = data.shape\n",
    "    header = f\"ncols {ncols}\\n\"\n",
    "    header += f\"nrows {nrows}\\n\"\n",
    "    #header += \"xllcorner -30934\\n\"  # x축의 최소값 HFC\n",
    "    #header += \"yllcorner 11140\\n\"  # y축의 최소값  HFC\n",
    "    header += \"xllcorner -31082\\n\"  # x축의 최소값 KMA\n",
    "    header += \"yllcorner 11395\\n\"  # y축의 최소값  KMA\n",
    "    header += \"cellsize 1000.0\\n\"  # 셀 크기\n",
    "    header += \"NODATA_value -9999.0\\n\"  # 누락된 데이터를 나타내는 값\n",
    "\n",
    "    np.savetxt(output_file, data, fmt=\"%.2f\", header=header, comments=\"\", delimiter=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72d0a5-9f55-4205-84d1-546b8c50b85b",
   "metadata": {},
   "source": [
    "#매선행시점(10분~180분) 최적 예측모델 이용하여 특정시점 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c2e6d1-df74-476a-93d1-b8f7fb857a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 17:01:02.584049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-06 17:01:03.357205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46578 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2023-08-06 17:01:03.357893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46711 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:b3:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 17:01:05.729650: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n",
      "2023-08-06 17:01:06.798292: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f08600b88c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f08601a2a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "(1, 640, 544, 1)\n",
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "(1, 640, 544, 4)\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "(1, 640, 544, 1)\n",
      "(625, 525, 18)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 이전에 정의한 data_preprocessing, data_postprocessing, prediction 함수를 사용합니다.\n",
    "\n",
    "qpf_all_each = np.empty((625, 525, 0))\n",
    "\n",
    "for j in range(10, 190, 10):\n",
    "    # 모델 불러오기\n",
    "    pre_model = f'models/model-best_fcst_{j}min.h5'\n",
    "    model = load_model(pre_model, custom_objects={\"CSI_custom\": CSI_custom, \"mae_custom\": mae_custom})\n",
    "    \n",
    "    # 데이터 준비하기\n",
    "    # FROM NUMPY\n",
    "    #filename = train_files0[22468]\n",
    "    #dataset0 = np.load(filename)\n",
    "    #FROM ASC (KMA mm/hr)\n",
    "    dataset0=qpe_data('2208082200')\n",
    "    dataset0.shape\n",
    "    dataset = np.transpose(dataset0,(1,2,0))\n",
    "    dataset = np.pad(dataset, [(7, 8), (9, 10), (0, 0)], mode='constant', constant_values=0)\n",
    "    feature0 = dataset[:, :, :] \n",
    "    feature = np.where(feature0 < 0., 0, feature0)\n",
    "\n",
    "    # 예측하기\n",
    "    nwcst = prediction(model, feature / 255.)\n",
    "    nwcst = nwcst * 255.\n",
    "\n",
    "    qpf_np = nwcst[7:632, 9:534]\n",
    "    #매 예측시점 자료를 ASCII GRID 형식으로 저장(EPSG518 기준)\n",
    "    output_file_path = 'QPF/QPF_'+date_nm+'-'+str(j)+'.asc'\n",
    "    save_as_ascii_grid(qpf_np, output_file_path)\n",
    "    #이미지 처리를 위한 3차원 배열 예측강우 생성\n",
    "    qpf_np = np.expand_dims(qpf_np, axis=-1)  # 차원을 늘리세요.\n",
    "    qpf_all_each = np.concatenate([qpf_all_each, qpf_np], axis=-1)\n",
    "    \n",
    "\n",
    "print(qpf_all_each.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab7548-ee98-485f-aca7-896b84ab5cf8",
   "metadata": {},
   "source": [
    "#재귀적 학습 기반 최적 예측모델 이용하여 특정시점 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc578e7c-d6ec-47b5-bd9e-cb0f2d77ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208082130\n",
      "2208082140\n",
      "2208082150\n",
      "2208082200\n",
      "(4, 625, 525)\n",
      "1 (4, 625, 525)\n",
      "2 (625, 525, 4)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "3 (18, 640, 544)\n",
      "(18, 625, 525)\n"
     ]
    }
   ],
   "source": [
    "#개쥐적 학습 기반 강우예측 모델을 이용한 예측강우 생산\n",
    "from keras.models import load_model\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "date_nm='202208082200'\n",
    "def prediction_rec(model_instance, input_data): #예측시 recursive 안함  \n",
    "    input_data = data_preprocessing(input_data)\n",
    "    pred = model_instance.predict(input_data)\n",
    "    pred=np.squeeze(pred)\n",
    "    nwcst = np.transpose(pred,(2,0,1))  \n",
    "    return nwcst\n",
    "\n",
    "model_rec = load_model('models/model-best_rec_180min_f.h5',custom_objects={\"CSI_custom\": CSI_custom,\"mae_custom\":mae_custom}) \n",
    "\n",
    "# 데이터 준비하기\n",
    "#FROM ASC (KMA mm/hr)\n",
    "dataset0=qpe_data('2208082200')\n",
    "print('1',dataset0.shape)\n",
    "dataset = np.transpose(dataset0,(1,2,0))\n",
    "print('2',dataset.shape)\n",
    "dataset = np.pad(dataset, [(7, 8), (9, 10), (0, 0)], mode='constant', constant_values=0)\n",
    "feature0 = dataset[:, :, :] \n",
    "feature = np.where(feature0 < 0., 0, feature0)\n",
    "nwcst = prediction_rec(model_rec, feature/255.)\n",
    "nwcst = nwcst*255\n",
    "print('3',nwcst.shape)\n",
    "#매 예측시점 자료를 ASCII GRID 형식으로 저장(EPSG518 기준)\n",
    "for j in range(10, 190, 10):\n",
    "    output_file_path = 'QPF/QPF_REC_'+date_nm+'-'+str(j)+'.asc'\n",
    "    k=int(j/10-1)\n",
    "    qpf_np = nwcst[k,7:632, 9:534]\n",
    "    save_as_ascii_grid(qpf_np, output_file_path)\n",
    "qpf_all = nwcst[:,7:632, 9:534]\n",
    "print(qpf_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334ed8c-ab6c-4510-92cd-903eb7d0c048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNET",
   "language": "python",
   "name": "unet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
